{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "# Suppress all warnings to keep the output clean\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Import essential libraries for data manipulation and visualization\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set a random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Import essential libraries for machine learning\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Import metrics and feature selection tools\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.feature_selection import mutual_info_classif\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def appendLabelToDf(dataFrame, columnToLabelBy, newColumnName):\n",
    "\n",
    "    # Override the threshold value for labeling\n",
    "    determinedThreshold = 50\n",
    "\n",
    "    # Create a new column with binary labels based on the threshold\n",
    "    dataFrame[newColumnName] = (dataFrame[columnToLabelBy] > determinedThreshold).astype(int)\n",
    "    return dataFrame, determinedThreshold\n",
    "\n",
    "\n",
    "def backwardEliminationLR(estimator, X_train, y_train, n_features_to_select):\n",
    "    \n",
    "    # Perform Recursive Feature Elimination (RFE) to select important features\n",
    "    selector = RFE(estimator, n_features_to_select = n_features_to_select, step=1)\n",
    "    selector = selector.fit(X_train, y_train)\n",
    "    \n",
    "    # Get the names of the selected features\n",
    "    selectedFeatures = selector.get_feature_names_out(X_train.columns)\n",
    "    \n",
    "    # Calculate the score of the model using the selected features\n",
    "    RFEscore = selector.score(X_train, y_train)\n",
    "    return selectedFeatures, RFEscore\n",
    "\n",
    "\n",
    "def plotEvaluation(y_pred, y_truth, setName):\n",
    "    yTestPredicted = y_pred\n",
    "    ygroundtruth = y_truth\n",
    "\n",
    "    # Confusion matrix for the given set\n",
    "    test_cm = confusion_matrix(ygroundtruth, yTestPredicted)\n",
    "\n",
    "    # Plot the confusion matrix\n",
    "    plt.figure(figsize = (4,3))\n",
    "    plt.subplot(1, 1, 1)\n",
    "    sns.heatmap(test_cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=['Class 0', 'Class 1'], yticklabels=['Class 0', 'Class 1'])\n",
    "    plt.xlabel('Predicted Class')\n",
    "    plt.ylabel('Actual Class')\n",
    "    plt.title(f'Confusion Matrix for the {setName} Set')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the plot as a high-resolution image\n",
    "    plt.savefig(f'figure-{setName}.png', dpi = 1000)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the filename of the Excel file containing the data frame\n",
    "dataFrameFileName = \"dataFrameHF.xlsx\"\n",
    "\n",
    "# Read the Excel file into a pandas DataFrame\n",
    "dataFrameHF = pd.read_excel(dataFrameFileName)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48.275862068965516"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the percentage of non-zero entries in the \"Prognosis Label\" column\n",
    "percentage_non_zero = 100 * (np.count_nonzero(dataFrameHF[\"Prognosis Label\"]) / dataFrameHF[\"Prognosis Label\"].shape[0])\n",
    "\n",
    "# Display the percentage of non-zero entries\n",
    "percentage_non_zero\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MIScoreBasedFeatureSelection(dataFrameHF, correlationIter, mutualInfoThreshold, test_size, testSizeCorrelationIter, random_state, isKfold, intersection = 0):\n",
    "    \"\"\"\n",
    "    Perform Mutual Information (MI) score-based feature selection on the given dataset.\n",
    "\n",
    "    Parameters:\n",
    "        dataFrameHF (DataFrame): The input dataset.\n",
    "        correlationIter (int): Number of iterations for correlation.\n",
    "        mutualInfoThreshold (float): Threshold for mutual information scores.\n",
    "        test_size (float): Proportion of the dataset to include in the test split.\n",
    "        testSizeCorrelationIter (float): Test size for correlation iterations.\n",
    "        random_state (int): Seed for random number generator.\n",
    "        isKfold (bool): Flag to indicate whether k-fold cross-validation(for leave-one-out-cross-validation) is used.\n",
    "        intersection (bool, optional): Flag to indicate whether to take intersection of features. Default is 0.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (X_train, X_test, y_train, y_test), selectedFeatures\n",
    "    \"\"\"\n",
    "\n",
    "    #### Train-test splitting ####\n",
    "    if not isKfold:\n",
    "        # Separate the target variable and feature variables\n",
    "        datasetY = dataFrameHF[[\"Prognosis Label\"]]\n",
    "        datasetX = dataFrameHF.drop([\"Prognosis Label\", \"Patient Name\"], axis=1)\n",
    "        # Split the dataset into training and test sets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(datasetX, datasetY, test_size=test_size, random_state=random_state, shuffle=False)\n",
    "    else:\n",
    "        # Use the provided dataset splits for k-fold cross-validation\n",
    "        (X_train, X_test, y_train, y_test) = dataFrameHF\n",
    "\n",
    "    ##################################################  \n",
    "          \n",
    "    # Define minimum and maximum thresholds for feature variance\n",
    "    min_threshold = 10**-5\n",
    "    max_threshold = 10**3\n",
    "\n",
    "    # Calculate variance and maximum values for each feature\n",
    "    variance_per_feature = X_train.var(axis=0)\n",
    "    max_per_feature = X_train.max(axis=0)\n",
    "    standardVar = np.abs(variance_per_feature / max_per_feature)\n",
    "    # Select features within the defined variance thresholds\n",
    "    featuresThresholded = X_train.columns[(standardVar >= min_threshold) & (standardVar <= max_threshold)].tolist()\n",
    "    X_train, X_test = X_train[featuresThresholded], X_test[featuresThresholded]\n",
    "\n",
    "    ##################################################\n",
    "        \n",
    "    ######################### FEATURE ELIMINATION-SELECTION ########################\n",
    "    MIscoresList = list()\n",
    "    # Calculate mutual information scores for all samples\n",
    "    mutual_info_scoresofAllSamples = mutual_info_classif(X_train, y_train, random_state=42)\n",
    "    # Select features with MI scores above the threshold\n",
    "    allSamplesCorrelateFeatures = X_train.columns[np.where(mutual_info_scoresofAllSamples > mutualInfoThreshold)[0]]\n",
    "    \n",
    "    for corrIter in range(correlationIter):\n",
    "        # Split the training data for correlation iterations\n",
    "        X_trainCorrelationIter, X_testCorrIter, y_trainCorrIter, y_testCorrIter = train_test_split(X_train, y_train, test_size=testSizeCorrelationIter, random_state=corrIter, shuffle=True)\n",
    "        \n",
    "        # Calculate mutual information scores for the split data\n",
    "        mutual_info_scores = mutual_info_classif(X_trainCorrelationIter, y_trainCorrIter, random_state=42)\n",
    "        MIscoresList.append(mutual_info_scores)\n",
    "\n",
    "    # Calculate mean mutual information scores across all iterations\n",
    "    MIscoresArr = np.array(MIscoresList)\n",
    "    MIscoresMeanArr = np.mean(MIscoresArr, axis=0)\n",
    "    # Select features with mean MI scores above the threshold\n",
    "    indices = np.where(MIscoresMeanArr > mutualInfoThreshold)[0]\n",
    "    featureNamesThresholded = X_train.columns[indices]\n",
    "\n",
    "    # Determine final selected features based on intersection if specified\n",
    "    if intersection:\n",
    "        selectedFeatures = list(set(featureNamesThresholded).intersection(allSamplesCorrelateFeatures))\n",
    "    else:\n",
    "        selectedFeatures = featureNamesThresholded\n",
    "\n",
    "    # Select the final features for training and test sets\n",
    "    X_train, X_test = X_train[selectedFeatures], X_test[selectedFeatures]\n",
    "    \n",
    "    return (X_train, X_test, y_train, y_test), selectedFeatures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelling(datasetSplits, penalty, C, solver):\n",
    "    \"\"\"\n",
    "    Train and evaluate a logistic regression model using the provided dataset splits and hyperparameters.\n",
    "\n",
    "    Parameters:\n",
    "        datasetSplits (tuple): A tuple containing the training and test sets (X_train, X_test, y_train, y_test).\n",
    "        penalty (str): The type of regularization to use ('l1', 'l2', etc.).\n",
    "        C (float): Inverse of regularization strength; smaller values specify stronger regularization.\n",
    "        solver (str): Algorithm to use in the optimization problem ('liblinear', 'saga', etc.).\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the training and test accuracies, and the trained classifier.\n",
    "    \"\"\"\n",
    "    X_train, X_test, y_train, y_test = datasetSplits\n",
    "    \n",
    "    # Apply robust scaling to the training data\n",
    "    scaler = RobustScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "\n",
    "    print(\"The classifier model is Logistic Regression:\")\n",
    "    classifier = LogisticRegression(C=C, max_iter=100, penalty=penalty, solver=solver)\n",
    "    \n",
    "    # Train the logistic regression model\n",
    "    classifier.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict on the training set and calculate training accuracy\n",
    "    y_pred = classifier.predict(X_train)\n",
    "    trainingAcc = accuracy_score(y_train, y_pred)\n",
    "    print(\"Training Accuracy ->\", round(trainingAcc, 2))\n",
    "\n",
    "    # Apply the same scaling to the test data\n",
    "    X_test = scaler.transform(X_test)\n",
    "    \n",
    "    # Predict on the test set and calculate test accuracy\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    testAcc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    print(\"Test Accuracy ->\", round(testAcc, 2))\n",
    "    print(\"-\")\n",
    "    \n",
    "    return (round(trainingAcc, 2), round(testAcc, 2)), classifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hold out method-Logistic Regression!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the mutual information threshold and number of iterations for correlation analysis\n",
    "MIThreshold = 0.105\n",
    "commoncorrIter = 15\n",
    "\n",
    "# Print the mutual information threshold for reference\n",
    "print(f\"MI: {MIThreshold}\")\n",
    "\n",
    "# Perform feature selection based on mutual information scores\n",
    "X_train, X_test, y_train, y_test, selectedFeaturesMI = MIScoreBasedFeatureSelection(\n",
    "    dataFrameHF, \n",
    "    commoncorrIter=int(commoncorrIter), \n",
    "    MIThreshold=MIThreshold, \n",
    "    test_size=0.35, \n",
    "    test_size_corr_iter=0.2, \n",
    "    random_state=42, \n",
    "    isKfold=False, \n",
    "    intersection=True\n",
    ")\n",
    "\n",
    "# Prepare the dataset splits for modeling\n",
    "datasetSplits = (X_train, X_test, y_train, y_test)\n",
    "\n",
    "# Define parameters for Lasso regularization\n",
    "CLasso = 1.2\n",
    "penaltyLasso = \"l1\"\n",
    "\n",
    "# Train a logistic regression model using Lasso regularization\n",
    "estimator = modelling(datasetSplits=datasetSplits, penalty=penaltyLasso, C=CLasso, solver=\"liblinear\")[1]\n",
    "\n",
    "# Number of features to select via backward elimination\n",
    "n_features_to_select = 5\n",
    "\n",
    "# Perform backward elimination if the number of selected features exceeds the desired number\n",
    "if len(selectedFeaturesMI) > n_features_to_select:\n",
    "    selectedFeatures, RFEscore = backwardEliminationLR(estimator, X_train, y_train, n_features_to_select)\n",
    "    selectedFeatures = list(selectedFeatures)\n",
    "else:\n",
    "    selectedFeatures = list(selectedFeaturesMI)\n",
    "\n",
    "# Define parameters for final logistic regression model with different regularization\n",
    "penaltyModelling = \"l2\"\n",
    "CModelling = 0.2\n",
    "\n",
    "# Print the final model parameters for reference\n",
    "print(f\"C: {CModelling}, penalty: {penaltyModelling}, n_features_to_select: {n_features_to_select}!\")\n",
    "\n",
    "# Train the final logistic regression model using the selected features\n",
    "(trainingAcc, testAcc), estimator = modelling(\n",
    "    datasetSplits=(X_train[selectedFeatures], X_test[selectedFeatures], y_train, y_test), \n",
    "    penalty=penaltyModelling, \n",
    "    C=CModelling, \n",
    "    solver=\"liblinear\"\n",
    ")\n",
    "\n",
    "# Print the training and test accuracies of the final model\n",
    "print(f\"train accuracy: {trainingAcc}, test accuracy: {testAcc}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-FOLD(K=29)(LEAVE ONE OUT Cross Validation) CV METHOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of splits for K-Fold cross-validation\n",
    "k = 29\n",
    "kf = KFold(n_splits=k, shuffle=False)  # You can change the random_state value\n",
    "\n",
    "# Prepare the dataset by separating the features and the target label\n",
    "datasetY = dataFrameHF[[\"Prognosis Label\"]]\n",
    "datasetX = dataFrameHF.drop([\"Prognosis Label\", \"Patient Name\"], axis=1)\n",
    "\n",
    "# Initialize a dictionary to store performance metrics\n",
    "performanceDict = dict()\n",
    "trainAcc, testAcc = 0, 0\n",
    "\n",
    "# Perform K-Fold cross-validation\n",
    "for train_index, test_index in kf.split(datasetX):\n",
    "    X_train, X_test = datasetX.iloc[train_index, :], datasetX.iloc[test_index, :]\n",
    "    y_train, y_test = datasetY.iloc[train_index, :], datasetY.iloc[test_index, :]\n",
    "\n",
    "    ############################## MI-Based Thresholding ##############################################################\n",
    "    (X_train_, X_test_, y_train_, y_test_), selectedFeaturesCorr = MIScoreBasedFeatureSelection(\n",
    "        (X_train, X_test, y_train, y_test), \n",
    "        commoncorrIter=int(commoncorrIter), \n",
    "        mutualInfoThreshold=MIThreshold, \n",
    "        test_size=0.35, \n",
    "        test_size_corr_iter=0.2, \n",
    "        random_state=42, \n",
    "        isKfold=True, \n",
    "        intersection=True\n",
    "    )\n",
    "    ############################## Feature Selection using LASSO - Backward Elimination ################################\n",
    "    (trainaccuracy_voting, testaccuracy_voting), estimator = modelling(\n",
    "        datasetSplits=(X_train_, X_test_, y_train_, y_test_), \n",
    "        penalty=penaltyLasso, \n",
    "        C=CLasso, \n",
    "        solver=\"liblinear\"\n",
    "    )\n",
    "\n",
    "    # Perform backward elimination if the number of selected features exceeds the desired number\n",
    "    if len(selectedFeaturesCorr) > n_features_to_select:\n",
    "        selectedFeatures, RFEscore = backwardEliminationLR(estimator, X_train_, y_train_, n_features_to_select)\n",
    "        selectedFeatures = list(selectedFeatures)\n",
    "    else:\n",
    "        selectedFeatures = list(selectedFeaturesCorr)\n",
    "    \n",
    "    ############################## Ultimate Modelling and Testing ####################################################\n",
    "    (trainingAcc, testAcc) = modelling(\n",
    "        datasetSplits=(X_train_[selectedFeatures], X_test_[selectedFeatures], y_train_, y_test_), \n",
    "        penalty=penaltyModelling, \n",
    "        C=CModelling, \n",
    "        solver=\"liblinear\"\n",
    "    )[0]\n",
    "    \n",
    "    trainingAccVoting, testAccVoting = trainingAcc / k, testAcc / k\n",
    "    \n",
    "    # Accumulate the training and test accuracies over the K-Fold splits\n",
    "    trainAcc += trainingAccVoting \n",
    "    testAcc += testAccVoting \n",
    "\n",
    "# Print the final training and test accuracies after K-Fold cross-validation\n",
    "print(f\"train accuracy: {trainAcc}, test accuracy: {testAcc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Box plots plotting to illustrate the distributions of features utilized in the modeling process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the target label and features of interest\n",
    "datasetY = dataFrameHF[[\"Prognosis Label\"]]\n",
    "datasetX = dataFrameHF.drop([\"Prognosis Label\", \"Patient Name\"], axis=1)[selectedFeatures]\n",
    "\n",
    "# Combine selected features and target label into a single dataframe\n",
    "dataFrameCombined = pd.concat([datasetX, datasetY], axis=1)\n",
    "\n",
    "# Create five separate boxplots, one for each selected feature\n",
    "for order, feature in enumerate(datasetX.columns):\n",
    "    # Create a new figure for each boxplot\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    \n",
    "    # Plot a boxplot for the current feature\n",
    "    sns.boxplot(x='Prognosis Label', y=feature, data=dataFrameCombined, palette={0: \"green\", 1: \"red\"})\n",
    "    \n",
    "    # Customize x-axis ticks and labels\n",
    "    plt.xticks(ticks=[0, 1], labels=['5-year mortality < 50%', '5-year mortality > 50%'], fontweight='bold')\n",
    "    \n",
    "    # Customize labels and title\n",
    "    plt.xlabel('Target', fontweight='bold')\n",
    "    plt.ylabel(f\"{feature} Values\", fontweight='bold')\n",
    "    plt.title(f'Boxplot of {feature} by Prognostic Label', fontweight='bold')\n",
    "    \n",
    "    # Customize y-axis ticks\n",
    "    plt.yticks(fontweight='bold')\n",
    "    \n",
    "    # Ensure tight layout\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the plot as a PNG file with high resolution\n",
    "    plt.savefig(f'Feature-{order + 1}_distribution.png', dpi=1000)\n",
    "    \n",
    "    # Display the plot\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
